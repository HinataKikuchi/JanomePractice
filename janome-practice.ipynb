{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36864bit123641f710d948cfac28fbe7508e2db7",
   "display_name": "Python 3.6.8 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<janome.tokenizer.Tokenizer at 0x20248cc82e8>"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "t = Tokenizer()\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '今日の気分は疲れてた気がする'\n",
    "tokens = t.tokenize(text, wakati=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "今日\nの\n気分\nは\n疲れ\nて\nた\n気\nが\nする\n"
     ]
    }
   ],
   "source": [
    "for token in tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['おいしい', 'ビール', 'を', '飲む'],\n",
       " ['コーヒー', 'を', '飲む'],\n",
       " ['おいしい', 'クラフト', 'ビール', 'を', '買う']]"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "t = Tokenizer()\n",
    "sentences = [\n",
    "    'おいしいビールを飲む','コーヒーを飲む','おいしいクラフトビールを買う'\n",
    "]\n",
    "words_list = []\n",
    "for sentence in sentences:\n",
    "    # 分かち書きした単語ごとの情報をwords_listに追加\n",
    "    words_list.append(list(t.tokenize(sentence, wakati=True)))\n",
    "words_list\n",
    "#t.tokenize(sentence, wakati=True)がイテレータなので表示がおかしくなるが\n",
    "# 別にnext(iter)として中身を取り出せば同じこと。まぁそれはめんどいんだけれども"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['おいしい', 'ビール', 'を', '飲む', 'コーヒー', 'クラフト', '買う']"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "unique_words=[]\n",
    "for words in words_list:\n",
    "    # それぞれの文から1単語ずつ抜き取る。被った単語は追加しない\n",
    "    for word in words:\n",
    "        if word not in unique_words:\n",
    "            unique_words.append(word)\n",
    "unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[1, 1, 1, 1, 0, 0, 0], [0, 0, 1, 1, 1, 0, 0], [1, 1, 1, 0, 0, 1, 1]]"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "# 検索分において先ほどの単語の出現回数をリストにしたい\n",
    "bow_list = []\n",
    "for words in words_list:\n",
    "    #文章のリストを回す。それぞれの文に対してbags_of_wordsを定義する\n",
    "    bag_of_words = []\n",
    "    for unique_word in unique_words:\n",
    "        #数を数えてbag_wordsに追加する\n",
    "        num = words.count(unique_word)\n",
    "        bag_of_words.append(num)\n",
    "    # bag_wordsを作り終わったらbow_listへ\n",
    "    bow_list.append(bag_of_words)\n",
    "bow_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.28768207245178085,\n",
       " 0.28768207245178085,\n",
       " 0.0,\n",
       " 0.28768207245178085,\n",
       " 0.6931471805599453,\n",
       " 0.6931471805599453,\n",
       " 0.6931471805599453]"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "# IDF( Inverse Document Frequency、逆文書頻度(出現頻度が高いとこの値は下がる))\n",
    "from math import log\n",
    "num_of_sentences = len(sentences)\n",
    "idf = []\n",
    "for i in range(len(unique_words)):\n",
    "    count = 0\n",
    "    for bow in bow_list:\n",
    "        if bow[i] > 0:\n",
    "            count += 1\n",
    "    idf.append(log((num_of_sentences + 1) / (count + 1)))\n",
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.42922735748392693,\n",
       " 0.5643823935199818,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "# TF-IDFを計算する\n",
    "# TF-IDFはTF（Term Frequency、単語の出現頻度)とIDFの掛け算\n",
    "# bow_list[1]は'コーヒーを飲むのbag_of_words'\n",
    "bow = bow_list[1]\n",
    "# 1文章の単語数\n",
    "num_of_words = sum(bow)\n",
    "tfidf = []\n",
    "#enumerate -> 第一戻り値:添え字     第二戻り値:要素\n",
    "# https://note.nkmk.me/python-enumerate-start/\n",
    "for i , value in enumerate(bow):\n",
    "    #tfを取得\n",
    "    tf = value / num_of_words\n",
    "    tfidf.append(tf * (idf[i] + 1))\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}